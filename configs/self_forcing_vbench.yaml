denoising_step_list:
- 1000
- 750
- 500
- 250
warp_denoising_step: true # need to remove - 0 in denoising_step_list if warp_denoising_step is true
num_frame_per_block: 3
model_name: /home/hangguo/pretrained/Wan2.1-T2V-1.3B
model_kwargs:
  local_attn_size: 12
  timestep_shift: 5.0
  sink_size: 3

dummy_forcing:
  denoise_start: 0 # we compute and perform dummy head pruning from the ARstep@ar_start and denoiseTime@denoise_start
  ar_start: 1
  local_context_length: 1 # the context length for local heads
  num_dummy: 180
  teacache_enabled: false
  teacache_threshold: 0.2

# inference
data_path: prompts/vbench/all_dimension.txt
extended_prompt_path: prompts/vbench/all_dimension_extended.txt
output_folder: videos # TODO add your official sampling dir here
inference_iter: -1
num_output_frames: 21 # latent num of frames 120
use_ema: true
seed: 0
num_samples: 5
save_with_index: true
global_sink: true
context_noise: 0
resolution: 480

generator_ckpt: /home/hangguo/pretrained/Self-Forcing/checkpoints/self_forcing_dmd.pt
lora_ckpt: ~

adapter:
  type: "lora"
  rank: 256                    # LoRA rank (typical values: 8, 16, 32, 64)
  alpha: 256                   # LoRA alpha (typically same as rank, but can be different)
  dropout: 0.0                # LoRA dropout rate
  dtype: "bfloat16"          # Data type for LoRA parameters: "bfloat16", "float16", "float32"
  verbose: false              # Whether to print all target module names